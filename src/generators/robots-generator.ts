// ============================================================
// robots.txt Generator
// Feature 2a: Auto-generate AI-optimized robots.txt
// ============================================================

import type { RobotsConfig } from '../types'
import { AI_CRAWLERS } from '../data/crawlers'

const DEFAULT_DISALLOW = ['/admin', '/api', '/private', '/_next', '/static']

/**
 * RobotsGenerator
 *
 * Generates a robots.txt file that explicitly allows AI crawlers
 * and optionally blocks specific ones (e.g., training bots).
 *
 * @example
 * ```ts
 * import { RobotsGenerator } from 'ai-visibility'
 * import fs from 'fs'
 *
 * const gen = new RobotsGenerator({
 *   allowAI: ['GPTBot', 'ClaudeBot', 'PerplexityBot'],
 *   blockAI: ['CCBot'],
 *   disallow: ['/admin', '/api'],
 *   sitemapUrl: 'https://mysite.com/sitemap.xml',
 * })
 *
 * fs.writeFileSync('./public/robots.txt', gen.generate())
 * ```
 */
export class RobotsGenerator {
    private config: Required<RobotsConfig>

    constructor(config: RobotsConfig = {}) {
        // Default: allow all known AI crawlers
        const allBotNames = AI_CRAWLERS.map((b) => b.name)

        this.config = {
            allowAI: config.allowAI ?? allBotNames,
            blockAI: config.blockAI ?? [],
            disallow: config.disallow ?? DEFAULT_DISALLOW,
            sitemapUrl: config.sitemapUrl ?? '',
            crawlDelay: config.crawlDelay ?? 0,
        }
    }

    /**
     * Generate the robots.txt content as a string.
     */
    generate(): string {
        const lines: string[] = []

        lines.push('# robots.txt — Generated by ai-visibility')
        lines.push('# https://github.com/yourusername/ai-visibility')
        lines.push('')

        // Blocked AI crawlers first
        if (this.config.blockAI.length > 0) {
            lines.push('# Blocked AI crawlers (training opt-out)')
            for (const bot of this.config.blockAI) {
                lines.push(`User-agent: ${bot}`)
                lines.push('Disallow: /')
                lines.push('')
            }
        }

        // Allowed AI crawlers
        const allowedBots = this.config.allowAI.filter(
            (b) => !this.config.blockAI.includes(b)
        )

        if (allowedBots.length > 0) {
            lines.push('# AI crawlers — explicitly allowed')
            for (const bot of allowedBots) {
                lines.push(`User-agent: ${bot}`)
                lines.push('Allow: /')
                if (this.config.crawlDelay > 0) {
                    lines.push(`Crawl-delay: ${this.config.crawlDelay}`)
                }
                lines.push('')
            }
        }

        // Default rules for all other bots
        lines.push('# Default rules')
        lines.push('User-agent: *')
        for (const path of this.config.disallow) {
            lines.push(`Disallow: ${path}`)
        }
        lines.push('')

        // Sitemap
        if (this.config.sitemapUrl) {
            lines.push(`Sitemap: ${this.config.sitemapUrl}`)
        }

        return lines.join('\n')
    }

    /**
     * Returns a pre-built robots.txt that allows ALL known AI crawlers.
     * Good for quick setup with zero configuration.
     */
    static allowAll(options: Partial<Pick<RobotsConfig, 'disallow' | 'sitemapUrl'>> = {}): string {
        return new RobotsGenerator({
            allowAI: AI_CRAWLERS.map((b) => b.name),
            blockAI: [],
            disallow: options.disallow ?? DEFAULT_DISALLOW,
            sitemapUrl: options.sitemapUrl,
        }).generate()
    }

    /**
     * Returns a robots.txt that blocks training bots (CCBot, GPTBot, etc.)
     * but allows search/indexing bots.
     */
    static blockTraining(options: Partial<Pick<RobotsConfig, 'disallow' | 'sitemapUrl'>> = {}): string {
        const trainingBots = AI_CRAWLERS
            .filter((b) => b.purpose === 'training')
            .map((b) => b.name)

        const searchBots = AI_CRAWLERS
            .filter((b) => b.purpose !== 'training')
            .map((b) => b.name)

        return new RobotsGenerator({
            allowAI: searchBots,
            blockAI: trainingBots,
            disallow: options.disallow ?? DEFAULT_DISALLOW,
            sitemapUrl: options.sitemapUrl,
        }).generate()
    }
}
